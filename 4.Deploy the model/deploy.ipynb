{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Deploy the model as a web service hosted on Azure Container Instances (ACI). \n",
        "\n",
        "1. Create the scoring script.\n",
        "1. Prepare an inference configuration.\n",
        "1. Deploy the previously trained model to the cloud.\n",
        "1. Consume data sample and test the web service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  1. Create the scoring script.\n",
        "\n",
        "Create the scoring script, called score.py, used by the web service call to show how to use the model.  \n",
        "You must include two required functions into the scoring script:\n",
        "* The `init()` function, which typically loads the model into a global object. \n",
        "    * This function is run only once when the Docker container is started. \n",
        "* The `run(input_data)` function uses the model to predict a value based on the input data. \n",
        "    * Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported.\n",
        "\n",
        "TIP: Documentation on Deploy a model to Azure Container Instances [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance/). Advanced entry script authoring [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#binary-data/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile score.py\n",
        "from azureml.contrib.services.aml_request import AMLRequest, rawhttp\n",
        "from azureml.contrib.services.aml_response import AMLResponse\n",
        "import json, os, io\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchxrayvision as xrv\n",
        "from torchvision import transforms\n",
        "from torchxrayvision.datasets import normalize\n",
        "import pydicom\n",
        "\n",
        "def init():\n",
        "    global modelx\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'pc-densenet-densenet-best.pt')\n",
        "    # print(model_path)\n",
        "    modelx = torch.load(model_path)\n",
        "\n",
        "# TIP:  To accept raw data, use the AMLRequest class in your entry script and add the @rawhttp decorator to the run() function\n",
        "#       more details in: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
        "# Note that despite the fact that we trained our model on PNGs, we would like to simulate\n",
        "# a scenario closer to the real world here and accept DICOMs into our score script. Here's how:\n",
        "@rawhttp\n",
        "def run(request):\n",
        "\n",
        "    if request.method == 'GET':\n",
        "        # For this example, just return the URL for GETs.\n",
        "        respBody = str.encode(request.full_path)\n",
        "        return AMLResponse(respBody, 200)\n",
        "\n",
        "    elif request.method == 'POST':\n",
        "        # For a real-world solution, you would load the data from reqBody\n",
        "        # and send it to the model. Then return the response.\n",
        "        try:\n",
        "\n",
        "            # For labels definition see file: '3.Build a model/trainingscripts/padchest_config.py'\n",
        "            pathologies_labels = ['Air Trapping', 'Aortic Atheromatosis', 'Aortic Elongation', 'Atelectasis',\n",
        "             'Bronchiectasis', 'Cardiomegaly', 'Consolidation', 'Costophrenic Angle Blunting', 'Edema', 'Effusion',\n",
        "             'Emphysema', 'Fibrosis', 'Flattened Diaphragm', 'Fracture', 'Granuloma', 'Hemidiaphragm Elevation',\n",
        "             'Hernia', 'Hilar Enlargement', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening',\n",
        "             'Pneumonia', 'Pneumothorax', 'Scoliosis', 'Tuberculosis']\n",
        "\n",
        "            # Read DICOM and apply photometric transformations\n",
        "            def read_and_rescale_image( filepath):\n",
        "                dcm = pydicom.read_file(filepath)\n",
        "                image = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "\n",
        "                def window_image(image, wc, ww):\n",
        "                    img_min = wc - ww // 2\n",
        "                    img_max = wc + ww // 2\n",
        "                    image[image < img_min] = img_min\n",
        "                    image[image > img_max] = img_max\n",
        "                    return image\n",
        "\n",
        "                image = window_image(image, dcm.WindowCenter, dcm.WindowWidth)\n",
        "                # Scales 16bit to [-1024 1024]\n",
        "                image = normalize(image, maxval=65535, reshape=True)\n",
        "                return image\n",
        "\n",
        "            file_bytes = request.files[\"image\"]\n",
        "\n",
        "            # Note that user can define this to be any other type of image\n",
        "            input_image = read_and_rescale_image(file_bytes)\n",
        "\n",
        "            preprocess = transforms.Compose([\n",
        "                xrv.datasets.XRayCenterCrop(),\n",
        "                xrv.datasets.XRayResizer(224)\n",
        "            ])\n",
        "\n",
        "            input_image = preprocess(input_image)\n",
        "            input_batch =  torch.from_numpy( input_image[np.newaxis,...] )\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                output = modelx(input_batch)\n",
        "\n",
        "            index = np.argsort( output.data.cpu().numpy() )\n",
        "            probability = torch.nn.functional.softmax(output[0], dim=0).data.cpu().numpy()\n",
        "\n",
        "            #Return the result\n",
        "            return {\"top_three_labels\":  [pathologies_labels[index[0][-1]], pathologies_labels[index[0][-2]], pathologies_labels[index[0][-3]] ],\n",
        "                \"probability\":[ round(probability[index[0][-1]]*100,2), round(probability[index[0][-2]]*100,2), round(probability[index[0][-3]]*100,2) ] }\n",
        "\n",
        "        except Exception as e:\n",
        "            result = str(e)\n",
        "            # return error message back to the client\n",
        "            return AMLResponse(json.dumps({\"error\": result}), 200)\n",
        "\n",
        "    else:\n",
        "        return AMLResponse(\"bad request\", 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Prepare an inference configuration.\n",
        "   * Create an environment object\n",
        "   * Create inference configuration to deploy the model as a web service using:\n",
        "      * The scoring file (`score.py`)\n",
        "         *  Use [AMLRequest](https://docs.microsoft.com/en-us/python/api/azureml-contrib-services/azureml.contrib.services.aml_request?view=azure-ml-py) and [AMLResponse](https://docs.microsoft.com/en-us/python/api/azureml-contrib-services/azureml.contrib.services.aml_response.amlresponse?view=azure-ml-py) classes to access RAW data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "import uuid\n",
        "from azureml.core.webservice import Webservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.environment import CondaDependencies\n",
        "\n",
        "# Connect to workspace\n",
        "from azureml.core import Workspace\n",
        "# Load workspace from config file\n",
        "# The workspace is the top-level resource for Azure Machine Learning, \n",
        "# providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "# Documentation: https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace\n",
        "ws = Workspace.from_config(path='../')\n",
        "print(\"Workspace:\",ws.name)\n",
        "\n",
        "# We create a light weight environment for inference \n",
        "# An Environment defines Python packages, environment variables, and Docker settings that are used in machine learning experiments,\n",
        "# including in data preparation, training, and deployment to a web service.\n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py\n",
        "rsna_env = Environment(name='rsna_demo-inference')\n",
        "\n",
        "# Set Environment:\n",
        "# The Environment manages application dependencies in an Azure Machine Learning \n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-defaults\")\n",
        "conda_dep.add_pip_package(\"azure-ml-api-sdk\")\n",
        "conda_dep.add_pip_package(\"torch\")\n",
        "conda_dep.add_pip_package(\"torchvision\")\n",
        "conda_dep.add_pip_package(\"torchxrayvision\")\n",
        "conda_dep.add_pip_package(\"pydicom\")\n",
        "# Add dependencies to environment \n",
        "rsna_env.python.conda_dependencies=conda_dep\n",
        "\n",
        "# Register model:\n",
        "# A model is the result of a Azure Machine learning training Run or some other model training process outside of Azure. \n",
        "# Regardless of how the model is produced, it can be registered in a workspace, where it is represented by a name and a version. \n",
        "# With the Model class, you can package models for use with Docker and deploy them as a real-time endpoint that can be used for inference requests.\n",
        "# Please set the version number accordingly the number of models that you have registered.\n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py\n",
        "model = Model(ws, 'padchest', version=6)\n",
        "\n",
        "# Set inference and ACI web service:\n",
        "# The inference configuration describes how to configure the model to make predictions. \n",
        "# It references to the scoring script (entry_script) and is used to locate all the resources required for the deployment. \n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=rsna_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Deploy in ACI\n",
        "   Deploy the model as ACI web service. Note that this step may take about 2-5 minutes to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637690309046
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set AciWebservice:\n",
        "# The AciWebservice class represents a machine learning model deployed as a web service endpoint on Azure Container Instances\n",
        "# The Inference configuration (inference_config) is an input parameter for Model deployment-related actions\n",
        "# Note that we trained using a GPU cluster and we set resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=2) respectively.\n",
        "# This will allow us to run inference in CPU and optimize memory. \n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py\n",
        "aci_config = AciWebservice.deploy_configuration(\n",
        "    cpu_cores=model.resource_configuration.cpu,\n",
        "    memory_gb=model.resource_configuration.memory_in_gb)\n",
        "\n",
        "service_name = 'padchest-aci'\n",
        "# Deploy:\n",
        "# The model is packaged (using Docker behind the scenes) as a real-time endpoint that is later used for inference requests.\n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py\n",
        "service = Model.deploy(workspace=ws, \n",
        "                       name=service_name, \n",
        "                       models=[model], \n",
        "                       inference_config=inference_config, \n",
        "                       deployment_config=aci_config,\n",
        "                       overwrite=True)\n",
        "\n",
        "service.wait_for_deployment(show_output=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4. Consume data sample and test the web service.\n",
        "We demonstrate how to consume DICOM images:\n",
        "* We trained our model from PNG files with 16 bits pixel depth. \n",
        "* To test the web service, we will send a DICOM file (16 bits).\n",
        "    * We will apply the image normalization implemented in the scoring script.\n",
        "\n",
        "To try out the model you would need a sample DICOM image. In order to obtain one, we recommend that you use one of the PADCHEST images you trained on and use the provided `png2dcm.py` script to generate a DICOM file out of it. You can also try using your own DICOM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637690677573
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pydicom\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Visualize converted DICOM file from the corresponding PNG file\n",
        "test_file = \"./sample_dicom.dcm\"\n",
        "dcm = pydicom.read_file(test_file)\n",
        "print(dcm)\n",
        "plt.imshow(dcm.pixel_array, cmap=plt.cm.bone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the model is deployed we can get the scoring web service's HTTP endpoint, which accepts REST client calls. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637690685083
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from azureml.core.webservice import Webservice\n",
        "import numpy as np\n",
        "\n",
        "# Webservice constructor is used to retrieve a cloud representation of a Webservice\n",
        "# object associated with the provided Workspace\n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice(class)?view=azure-ml-py\n",
        "service = Webservice(name='padchest-aci', workspace=ws)\n",
        "\n",
        "# Get the web service HTTP endpoint.\n",
        "# This endpoint can be shared with anyone who wants to test the web service or integrate it into an application.\n",
        "uri = service.scoring_uri\n",
        "print(uri)\n",
        "\n",
        "files = {'image': open(test_file, 'rb').read()}\n",
        "\n",
        "# Send the DICOM as a raw HTTP request and obtain results from endpoint.\n",
        "response = requests.post(uri, files=files)\n",
        "print(\"output:\", response.content)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "6fa67edf6d87aa13ac525a1287441ea8850f1587e23cc2fe3e03f5742d416d61"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
